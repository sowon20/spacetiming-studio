from __future__ import annotations
from .memory_store import save_memory_events, load_recent_memories
from .imported_memory_loader import load_imported_memories
from .soul_loader import (
    build_core_system_prompt,
    build_timeline_context,
)
from .prompt_loader import load_base_system_prompt

import os
import json
import re
import logging
from typing import Optional
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from director_core.config_manager import apply_config_updates

from .models import (
    AnalyzeRequest,
    AnalyzeResponse,
    EpisodeHint,
    IntentLiteral,
)

logger = logging.getLogger(__name__)

# ê°„ë‹¨í•œ vision ë¶„ì„ (Gemini + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
def analyze_vision(req: AnalyzeRequest) -> AnalyzeResponse:
    media = getattr(req, "media", []) or []
    caption = (getattr(req, "text", None) or "").strip()

    # mediaì—ì„œ file_urlì„ í•˜ë‚˜ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ telegram_file_idë¼ë„ ì‚¬ìš©)
    file_url = None
    if media:
        item = media[0] or {}
        file_url = item.get("file_url") or item.get("telegram_file_id")

    if not file_url:
        reply = "ì´ë¯¸ì§€ëŠ” ë°›ì•˜ëŠ”ë°, ì„œë²„ì—ì„œ íŒŒì¼ ì£¼ì†Œë¥¼ ëª» ì°¾ê² ì–´. í•œ ë²ˆë§Œ ë‹¤ì‹œ ë³´ë‚´ì£¼ê±°ë‚˜, í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ë„ ê°™ì´ ì ì–´ì¤„ë˜?"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ íŒŒì¼ URL ì—†ìŒ",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    image_bytes = None
    mime_type = "image/jpeg"
    try:
        import httpx

        with httpx.Client(timeout=10.0) as client:
            resp = client.get(file_url)
            resp.raise_for_status()
            image_bytes = resp.content
            ct = resp.headers.get("content-type")
            if ct:
                mime_type = ct.split(";")[0].strip()
    except Exception:
        logger.exception("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜¨ ê²ƒ ê°™ì€ë°, ì„œë²„ì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ë‹¤ê°€ ìê¾¸ ì‹¤íŒ¨í•´. ì ì‹œ í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œë§Œ ë„ì™€ì¤„ê²Œ."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # LLMì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•ˆë‚´ë§Œ
    if not is_llm_available():
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ì–´. ì§€ê¸ˆì€ ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ì„ ëª» ì¨ì„œ, í…ìŠ¤íŠ¸ ì„¤ëª… ìœ„ì£¼ë¡œë§Œ ê°™ì´ ë³¼ ìˆ˜ ìˆì–´. ì‚¬ì§„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ê¸€ë¡œ ë§í•´ì¤„ë˜?"
        if caption:
            reply += f"\n\në„¤ê°€ ë‚¨ê¸´ ë©”ëª¨: {caption}"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ìˆ˜ì‹ (LLM ì—†ìŒ)",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # Geminië¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë¶„ì„
    try:
        model = get_gemini_model()

        base_prompt = (
            "ì´ ì´ë¯¸ì§€ë¥¼ ì°¨ë¶„í•˜ê²Œ ë¶„ì„í•´ì¤˜. í•œêµ­ì–´ë¡œ 3~6ë¬¸ì¥ ì •ë„ë¡œ í•µì‹¬ë§Œ ì„¤ëª…í•´ì¤˜. "
            "êµ¬ë„, ë¶„ìœ„ê¸°, ìƒì§•ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ” í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ë§í•´ì¤˜."
        )
        if caption:
            base_prompt += f"\n\nì‚¬ìš©ì ìš”ì²­/ë©”ëª¨: {caption}"

        parts = [
            base_prompt,
            {
                "mime_type": mime_type,
                "data": image_bytes,
            },
        ]

        chat = model.start_chat()
        resp = chat.send_message(parts)
        reply_text = (getattr(resp, "text", None) or "").strip()

        if not reply_text:
            reply_text = "ì´ë¯¸ì§€ëŠ” ì˜ ë´¤ì–´. ëŠë‚Œì€ ì¢‹ì€ë°, ëª¨ë¸ì´ ë§ì„ ì˜ ëª» êº¼ë‚´ê³  ìˆì–´. ê¶ê¸ˆí•œ ê±¸ ë§ë¡œ ë” ë¬¼ì–´ë´ì¤˜. ğŸ™‚"

        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ",
            intent="casual_chat",
            reply=reply_text,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={"gemini_raw_text": getattr(resp, "text", None)},
        )

    except Exception:
        logger.exception("Gemini vision í˜¸ì¶œ ì¤‘ ì—ëŸ¬")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ëŠ”ë°, ì§€ê¸ˆ ì´ë¯¸ì§€ ë¶„ì„ ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚œ ê²ƒ ê°™ì•„. ì ê¹ì€ í…ìŠ¤íŠ¸ë¡œë§Œ ê°™ì´ ë³´ì."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì—ëŸ¬",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import google.generativeai as genai
except Exception:
    genai = None


def get_gemini_api_key() -> Optional[str]:
    # ë£¨íŠ¸ .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ GEMINI_API_KEY ì‚¬ìš©
    return os.getenv("GEMINI_API_KEY")


def is_llm_available() -> bool:
    return bool(get_gemini_api_key() and genai is not None)


def get_gemini_model():
    api_key = get_gemini_api_key()
    if not api_key or genai is None:
        raise RuntimeError("GEMINI_API_KEYê°€ ì—†ê±°ë‚˜ google-generativeaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.0-flash", system_instruction=SYSTEM_PROMPT)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(
    title="Spacetime Director Core",
    version="0.1.0",
)

# ì‚¬ì´ë“œë°” / í…”ë ˆê·¸ë¨ì—ì„œ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ CORS ì—¬ìœ ìˆê²Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "llm_available": is_llm_available(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ë¶„ì„ ë¡œì§ (ì§€ê¸ˆì€ í…ìŠ¤íŠ¸ chatë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = load_base_system_prompt()

core_soul_prompt = build_core_system_prompt()
if core_soul_prompt:
    SYSTEM_PROMPT = SYSTEM_PROMPT + "\n\n" + core_soul_prompt


def load_recent_dialogue(user_id: str, limit: int = 40) -> str:
    """
    portal_history/{user_id}.chat.jsonl ì—ì„œ ìµœê·¼ ëŒ€í™” ëª‡ ì¤„ì„ ì½ì–´ì„œ
    LLM í”„ë¡¬í”„íŠ¸ì— ë¶™ì¼ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë§Œë“ ë‹¤.
      ì˜ˆì‹œ:
        {"role": "user", "content": "..."}
        {"role": "assistant", "content": "..."}
    """
    try:
        history_path = Path("portal_history") / f"{user_id}.chat.jsonl"
        if not history_path.exists():
            return ""

        lines = history_path.read_text(encoding="utf-8").splitlines()
        if not lines:
            return ""

        # ğŸ”¹ ìµœê·¼ limitê°œë§Œ ì‚¬ìš© (í—›ì†Œë¦¬/ê°íƒ„ì‚¬ ì„ì—¬ë„ ê¸¸ì´ë¡œ ë°€ì–´ë¶™ì´ê¸°)
        recent = lines[-limit:]

        blocks: list[str] = []
        for line in recent:
            try:
                obj = json.loads(line)
            except Exception:
                continue

            role = (obj.get("role") or obj.get("speaker") or "user").strip()
            content = (obj.get("content") or obj.get("text") or "").strip()
            if not content:
                continue

            prefix = "assistant" if role == "assistant" else "user"
            blocks.append(f"{prefix}: {content}")

        if not blocks:
            return ""

        return "[Recent dialogue]\n" + "\n".join(blocks)

    except Exception:
        logger.exception("ìµœê·¼ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ (ë¬´ì‹œí•˜ê³  ì§„í–‰)")
        return ""

def analyze_text_with_llm(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    mode == chat ì¸ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜.
    """
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="textê°€ ë¹„ì–´ ìˆì–´. ë¶„ì„í•  ë‚´ìš©ì´ ì—†ì–´.")

    # 1) LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Gemini í˜¸ì¶œ (ì—ëŸ¬ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ìë™ ì „í™˜)
    if is_llm_available():
        user_id = getattr(req, "user_id", "default_user")

        # ğŸ”¹ ìµœê·¼ ì¥ê¸° ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ê¸°
        try:
            recent_memories = load_recent_memories(user_id)
        except Exception:
            logger.exception("ìµœê·¼ ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")
            recent_memories = []

        # ğŸ”¹ ë¶ˆíƒ„ chatGPT ë°©ì—ì„œ ê°€ì ¸ì˜¨ ì˜¤ë˜ëœ ê¸°ì–µë“¤ë„ ê°™ì´ ì„ì–´ ì¤€ë‹¤.
        try:
            imported_memories = load_imported_memories(limit=30)
            for m in imported_memories:
                m.setdefault("tags", [])
                if "burned_room" not in m["tags"]:
                    m["tags"].append("burned_room")
            recent_memories.extend(imported_memories)
        except Exception:
            logger.exception("ë¶ˆíƒ„ë°© imported memories í•©ì¹˜ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")

        try:
            model = get_gemini_model()

            # recent_memoriesë¥¼ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ í’€ì–´ì¤Œ
            memories_block = ""
            if recent_memories:
                lines = [
                    f"- ({m.get('type')}) {m.get('summary')}"
                    for m in recent_memories
                ]
                memories_block = "Recent long-term memories:\n" + "\n".join(lines)

            # íƒ€ì„ë¼ì¸ ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
            timeline_block = build_timeline_context(user_text=text, max_events=5)

            user_prompt = f"ì‚¬ìš©ìì˜ ë§:\n{text}"

            # ğŸ”¹ ìµœê·¼ ëŒ€í™” ë¸”ë¡ ë¶™ì´ê¸° (ì™•ë³µ 20ë²ˆ = 40í„´)
            recent_dialogue_block = load_recent_dialogue(user_id, limit=40)

            combined_parts: list[str] = []
            if recent_dialogue_block:
                combined_parts.append(recent_dialogue_block)
            if memories_block:
                combined_parts.append("[Recent memories]")
                combined_parts.append(memories_block)
            if timeline_block:
                combined_parts.append("[Timeline context]")
                combined_parts.append(timeline_block)
            combined_parts.append("[Director message]")
            combined_parts.append(user_prompt)

            combined_prompt = "\n\n".join(combined_parts)

            chat_session = model.start_chat()
            chat = chat_session.send_message(combined_prompt)

            raw_text = chat.text or ""
            raw = {"text": raw_text}

            # JSON íŒŒì‹± ì‹œë„
            try:
                match = re.search(r"```json(.*?)```", raw_text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = raw_text
                obj = json.loads(json_str)
            except Exception:
                # JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´, LLMì´ ëŒë ¤ì¤€ ìì—°ì–´ ë‹µë³€ ìì²´ë¥¼ ì‚¬ìš©í•œë‹¤.
                logger.exception("LLM JSON íŒŒì‹± ì‹¤íŒ¨, raw_textë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
                cleaned = (raw_text or "").strip()
                if not cleaned:
                    cleaned = "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š"

                obj = {
                    "summary": text[:60],
                    "intent": "unknown",
                    "reply": cleaned,
                    "episode_hint": {
                        "should_create_episode": False,
                        "suggested_title": None,
                        "suggested_plan": None,
                        "priority": "normal",
                    },
                    "memory_events": [],
                    "config_updates": [],
                }

            # ğŸ”¹ config_updates ì²˜ë¦¬
            config_updates = obj.get("config_updates") or []
            try:
                apply_config_updates(config_updates)
            except Exception:
                logger.exception("config_updates ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")

            # ğŸ”¹ memory_events ì²˜ë¦¬
            memory_events = obj.get("memory_events") or []
            if not memory_events:
                memory_events = [
                    {
                        "type": "observation",
                        "importance": 0.6,
                        "tags": ["chat"],
                        "summary": text[:60],
                        "source": "telegram",
                        "media_refs": [],
                        "raw": {"text": text},
                    }
                ]
            try:
                save_memory_events(user_id, memory_events)
            except Exception:
                logger.exception("memory_events ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")

            summary = obj.get("summary", text[:60])
            intent = obj.get("intent", "unknown")
            reply = obj.get("reply", "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š")

            eh = obj.get("episode_hint") or {}
            episode_hint = EpisodeHint(
                should_create_episode=bool(eh.get("should_create_episode", False)),
                suggested_title=eh.get("suggested_title"),
                suggested_plan=eh.get("suggested_plan"),
                priority=eh.get("priority") or "normal",
            )

            safe_intent: IntentLiteral
            if intent in [
                "veo_prompt",
                "casual_chat",
                "daily_log",
                "todo",
                "question",
                "unknown",
            ]:
                safe_intent = intent  # type: ignore
            else:
                safe_intent = "unknown"

            return AnalyzeResponse(
                ok=True,
                mode=req.mode,
                summary=summary,
                intent=safe_intent,
                reply=reply,
                episode_hint=episode_hint,
                raw_model_output=raw,
            )
        except Exception:
            # LLM ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ë„˜ê¸´ë‹¤.
            logger.exception("LLM ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ, ê·œì¹™ ê¸°ë°˜ fallback ì‚¬ìš©")
            # ì•„ë˜ì˜ ê·œì¹™ ê¸°ë°˜ fallback ë¡œì§ìœ¼ë¡œ ë‚´ë ¤ê°€ë„ë¡ í•œë‹¤.

    # 2) LLM ë¶ˆê°€ì¼ ë•Œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ fallback
    lowered = text.lower()
    intent: IntentLiteral
    if any(k in lowered for k in ["ì˜ìƒ", "ë¹„ë””ì˜¤", "ì”¬", "ì¥ë©´", "shot"]):
        intent = "veo_prompt"
    elif any(k in lowered for k in ["í•´ì•¼", "í•  ì¼", "todo", "ê¸°ì–µí•´ì¤˜"]):
        intent = "todo"
    elif any(k in lowered for k in ["ì™œ", "ì–´ë–»ê²Œ", "?", "ê¶ê¸ˆ"]):
        intent = "question"
    else:
        intent = "casual_chat"

    summary = text[:60]
    reply = (
        "ì§€ê¸ˆì€ ë‚´ ë¨¸ë¦¬(LLM)ê°€ ì ê¹ ëŠê²¨ì„œ ê¹Šê²Œ ê°™ì´ ìƒê°í•˜ì§„ ëª»í•´. "
        "ê·¸ë˜ë„ ë„¤ ë§ì€ ê·¸ëŒ€ë¡œ ê¸°ë¡í•´ë‘˜ê²Œ. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì—°ê²°ë˜ë©´ ê·¸ë•Œ ì œëŒ€ë¡œ ê°™ì´ ë³´ì."
    )

    episode_hint = EpisodeHint(
        should_create_episode=(intent == "veo_prompt"),
        suggested_title=None,
        suggested_plan=None,
        priority="normal",
    )

    return AnalyzeResponse(
        ok=True,
        mode=req.mode,
        summary=summary,
        intent=intent,
        reply=reply,
        episode_hint=episode_hint,
        raw_model_output={},
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/director/analyze", response_model=AnalyzeResponse)
def analyze(req: AnalyzeRequest):
    """
    ê³µí†µ ë‡Œ ì—”ë“œí¬ì¸íŠ¸.
    ì§€ê¸ˆì€ mode == chat (í…ìŠ¤íŠ¸)ë§Œ ì§€ì›.
    """
    # vision ëª¨ë“œ ë¶„ê¸°
    if req.mode == "vision":
        return analyze_vision(req)

    # ê¸°ë³¸: í…ìŠ¤íŠ¸(chat) ë¶„ì„
    return analyze_text_with_llm(req)