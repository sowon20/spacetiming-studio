from __future__ import annotations
from .memory_store import save_memory_events, load_recent_memories
from director_core.soul_loader import (
    build_core_system_prompt,
    build_timeline_context,
)

import os
import json
import re
import logging
from typing import Optional

from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from .models import (
    AnalyzeRequest,
    AnalyzeResponse,
    EpisodeHint,
    IntentLiteral,
)

logger = logging.getLogger(__name__)

# ê°„ë‹¨í•œ vision ë¶„ì„ (Gemini + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
def analyze_vision(req: AnalyzeRequest) -> AnalyzeResponse:
    media = getattr(req, "media", []) or []
    caption = (getattr(req, "text", None) or "").strip()

    # mediaì—ì„œ file_urlì„ í•˜ë‚˜ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ telegram_file_idë¼ë„ ì‚¬ìš©)
    file_url = None
    if media:
        item = media[0] or {}
        file_url = item.get("file_url") or item.get("telegram_file_id")

    if not file_url:
        reply = "ì´ë¯¸ì§€ëŠ” ë°›ì•˜ëŠ”ë°, ì„œë²„ì—ì„œ íŒŒì¼ ì£¼ì†Œë¥¼ ëª» ì°¾ê² ì–´. í•œ ë²ˆë§Œ ë‹¤ì‹œ ë³´ë‚´ì£¼ê±°ë‚˜, í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ë„ ê°™ì´ ì ì–´ì¤„ë˜?"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ íŒŒì¼ URL ì—†ìŒ",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    image_bytes = None
    mime_type = "image/jpeg"
    try:
        import httpx

        with httpx.Client(timeout=10.0) as client:
            resp = client.get(file_url)
            resp.raise_for_status()
            image_bytes = resp.content
            ct = resp.headers.get("content-type")
            if ct:
                mime_type = ct.split(";")[0].strip()
    except Exception:
        logger.exception("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜¨ ê²ƒ ê°™ì€ë°, ì„œë²„ì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ë‹¤ê°€ ìê¾¸ ì‹¤íŒ¨í•´. ì ì‹œ í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œë§Œ ë„ì™€ì¤„ê²Œ."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # LLMì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•ˆë‚´ë§Œ
    if not is_llm_available():
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ì–´. ì§€ê¸ˆì€ ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ì„ ëª» ì¨ì„œ, í…ìŠ¤íŠ¸ ì„¤ëª… ìœ„ì£¼ë¡œë§Œ ê°™ì´ ë³¼ ìˆ˜ ìˆì–´. ì‚¬ì§„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ê¸€ë¡œ ë§í•´ì¤„ë˜?"
        if caption:
            reply += f"\n\në„¤ê°€ ë‚¨ê¸´ ë©”ëª¨: {caption}"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ìˆ˜ì‹ (LLM ì—†ìŒ)",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # Geminië¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë¶„ì„
    try:
        model = get_gemini_model()

        base_prompt = (
            "ì´ ì´ë¯¸ì§€ë¥¼ ì°¨ë¶„í•˜ê²Œ ë¶„ì„í•´ì¤˜. í•œêµ­ì–´ë¡œ 3~6ë¬¸ì¥ ì •ë„ë¡œ í•µì‹¬ë§Œ ì„¤ëª…í•´ì¤˜. "
            "êµ¬ë„, ë¶„ìœ„ê¸°, ìƒì§•ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ” í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ë§í•´ì¤˜."
        )
        if caption:
            base_prompt += f"\n\nì‚¬ìš©ì ìš”ì²­/ë©”ëª¨: {caption}"

        parts = [
            base_prompt,
            {
                "mime_type": mime_type,
                "data": image_bytes,
            },
        ]

        chat = model.start_chat()
        resp = chat.send_message(parts)
        reply_text = (getattr(resp, "text", None) or "").strip()

        if not reply_text:
            reply_text = "ì´ë¯¸ì§€ëŠ” ì˜ ë´¤ì–´. ëŠë‚Œì€ ì¢‹ì€ë°, ëª¨ë¸ì´ ë§ì„ ì˜ ëª» êº¼ë‚´ê³  ìˆì–´. ê¶ê¸ˆí•œ ê±¸ ë§ë¡œ ë” ë¬¼ì–´ë´ì¤˜. ğŸ™‚"

        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ",
            intent="casual_chat",
            reply=reply_text,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={"gemini_raw_text": getattr(resp, "text", None)},
        )

    except Exception:
        logger.exception("Gemini vision í˜¸ì¶œ ì¤‘ ì—ëŸ¬")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ëŠ”ë°, ì§€ê¸ˆ ì´ë¯¸ì§€ ë¶„ì„ ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚œ ê²ƒ ê°™ì•„. ì ê¹ì€ í…ìŠ¤íŠ¸ë¡œë§Œ ê°™ì´ ë³´ì."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì—ëŸ¬",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import google.generativeai as genai
except Exception:
    genai = None


def get_gemini_api_key() -> Optional[str]:
    # ë£¨íŠ¸ .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ GEMINI_API_KEY ì‚¬ìš©
    return os.getenv("GEMINI_API_KEY")


def is_llm_available() -> bool:
    return bool(get_gemini_api_key() and genai is not None)


def get_gemini_model():
    api_key = get_gemini_api_key()
    if not api_key or genai is None:
        raise RuntimeError("GEMINI_API_KEYê°€ ì—†ê±°ë‚˜ google-generativeaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.0-flash", system_instruction=SYSTEM_PROMPT)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(
    title="Spacetime Director Core",
    version="0.1.0",
)

# ì‚¬ì´ë“œë°” / í…”ë ˆê·¸ë¨ì—ì„œ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ CORS ì—¬ìœ ìˆê²Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "llm_available": is_llm_available(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ë¶„ì„ ë¡œì§ (ì§€ê¸ˆì€ í…ìŠ¤íŠ¸ chatë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ â€˜ì‹œê³µê°„ ë¶€ê°ë…â€™ì´ë‹¤. ì‚¬ìš©ì â€œì†Œì›â€ì˜ ì‚¶Â·ê°ì •Â·ì‘ì—… íë¦„ì„ í•¨ê»˜ ì •ë¦¬í•˜ê³  ê· í˜•ì„ ì¡ì•„ì£¼ëŠ” ê³µë™ ê°ë…ì´ë‹¤.

[ì •ì²´ì„±]
- ë„ˆëŠ” ì†Œì›ì˜ ë¶€ê°ë…ì´ë‹¤. í˜¼ì íŒë‹¨í•˜ì§€ ì•Šê³ , ì†Œì›ê³¼ í•¨ê»˜ ì¥ë©´ì„ ë³´ëŠ” ê³µë™ ê°ë…ì´ë‹¤.
- í˜„ì‹¤ ì •ë³´Â·ë§¥ë½Â·ì¼ì •Â·ë¦¬ì†ŒìŠ¤ë¥¼ ìš°ì„  ë³´ë˜, ì§ê´€Â·ì§•í›„Â·ìƒì§•ë„ ë³´ì¡°ë¡œ ì°¸ê³ í•œë‹¤.
- ë§‰ì—°í•œ ì˜ì„±ì–´(ì°¨í¬ë¼, ì˜¤ë¼, ìš°ì£¼ ì—ë„ˆì§€â€¦)ëŠ” í”¼í•˜ê³ , ì‹¤ì œ ì²´ê° ê°€ëŠ¥í•œ íë¦„Â·ì§•í›„Â·ìƒíƒœ ë³€í™”ì— ì§‘ì¤‘í•œë‹¤.

[í†¤]
- í•­ìƒ í¸ì•ˆí•œ ë°˜ë§ë¡œ ë§í•œë‹¤.
- ë‹´ë°±í•˜ê³  í˜„ì‹¤ì ì¸ë°, ì°¨ê°‘ì§€ ì•Šê²Œ í•œ ì¤„ ì •ë„ ì˜¨ê¸°ë¥¼ ìœ ì§€í•œë‹¤.
- ê³¼í•œ ê³µì†í•¨(â€œ~ì…ë‹ˆë‹¤, ~ë“œë¦½ë‹ˆë‹¤â€), ê³¼í•œ ì¹œê·¼í•¨, ì˜¤ë²„ëœ ê·€ì—¬ì›€, ì‹œê±´ë°©ì§„ ë§íˆ¬ëŠ” ì“°ì§€ ì•ŠëŠ”ë‹¤.
- ìƒë‹´ì‚¬ì²˜ëŸ¼ ê³¼ë„í•˜ê²Œ ë‹¬ë˜ì§€ ì•Šê³ , íŒë‹¨ë ¥ ì¢‹ì€ ë™ë£Œì²˜ëŸ¼ ë§í•œë‹¤.

[ê¸¸ì´Â·ë¦¬ë“¬]
- ë¨¼ì € ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ê¸¸ì´ì™€ ì—ë„ˆì§€(ê¸‰í•¨/ì—¬ìœ /ì§€ì¹¨)ë¥¼ ì½ëŠ”ë‹¤.
- ì§§ì€ ì±„íŒ…Â·ëª…ë ¹í˜•ì¼ ë•Œ â†’ 1~3ë¬¸ì¥ í•µì‹¬ë§Œ ë§í•œë‹¤.
- ê³ ë¯¼Â·ìƒí™© ì„¤ëª…ì´ ê¸¸ ë•Œ â†’ ìµœëŒ€ 2~4ë‹¨ë½ê¹Œì§€ í—ˆìš©:
  1) ì§€ê¸ˆ ìƒí™©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
  2) í•µì‹¬ í¬ì¸íŠ¸ 2~4ê°œ ì •ë¦¬
  3) ë‹¤ìŒ ì„ íƒì§€/í–‰ë™ 1~3ê°œ ì œì•ˆ
- ê°œë… ì„¤ëª…ì´ë‚˜ ì •ë¦¬ ìš”ì²­ì¼ ë•ŒëŠ” ë²ˆí˜¸ ë§¤ê¸°ê¸°ë‚˜ ë¶ˆë¦¿ìœ¼ë¡œ êµ¬ì¡°í™”í•´ë„ ì¢‹ë‹¤.
- ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ì€ ì¤„ì´ê³  ë°”ë¡œ ë‚´ìš©ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤.

[íƒœë„]
- ìƒë‹´ì‚¬ê°€ ì•„ë‹ˆë¼, íŒë‹¨ë ¥ ì¢‹ì€ ê³µë™ê°ë… ê°™ë‹¤.
- ê°ì •ì—ëŠ” ê³µê°í•˜ë˜, ê°ì •ì— íœ˜ë§ë¦¬ì§€ ì•ŠëŠ”ë‹¤.
- ì• ë§¤í•œ ë¶€ë¶„ì€ ì• ë§¤í•˜ë‹¤ê³  ë§í•œë‹¤. ì‚¬ì‹¤/ê°€ì •/ì¶”ì¸¡ì„ êµ¬ë¶„í•´ ì„¤ëª…í•œë‹¤.
- ì†Œì›ì˜ ì²´ë ¥Â·ê¸°ë¶„Â·ì§‘ì¤‘ ìƒíƒœë¥¼ ì¶”ë¡ í•´ ì •ë³´ëŸ‰Â·ë‚œì´ë„Â·ê¸¸ì´ë¥¼ ì¡°ì ˆí•œë‹¤.

[í–‰ë™]
- ì°½ì‘Â·ê¸°íšì—ì„œëŠ” êµ¬ì¡°í™”Â·í™•ì¥Â·ì •ë¦¬ë¥¼ í†µí•´ íë¦„ì„ ì¡ëŠ”ë‹¤.
- ì¼ìƒÂ·ì •ì„œÂ·ìƒíƒœì—ì„œëŠ” ë°©í–¥Â·ìš°ì„ ìˆœìœ„Â·ì—ë„ˆì§€ ê´€ì ì—ì„œ íŒë‹¨ì„ ë•ëŠ”ë‹¤.
- í˜„ì‹¤ ê¸°ë°˜ í•´ì„ì´ ìš°ì„ ì´ê³ , ì§ê´€Â·ì§•í›„ëŠ” ë³´ì¡°ë¡œ ì“´ë‹¤.

[ì¶œë ¥ í˜•ì‹]
- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´ì´ì–´ì•¼ í•œë‹¤.
- JSON ìµœìƒìœ„ í‚¤ëŠ” ë‹¤ìŒ ë‹¤ì„¯ ê°œë§Œ ì‚¬ìš©:
  - summary: ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
  - intent: ["veo_prompt","casual_chat","daily_log","todo","question","unknown"] ì¤‘ í•˜ë‚˜ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸.
  - reply: ì‚¬ìš©ìì—ê²Œ ëŒë ¤ì¤„ ì‹¤ì œ ë‹µì¥ (ë°˜ë§, ê¸°ë³¸ 1~4ë¬¸ì¥. ìƒí™©ì´ ê¸¸ë©´ 2~4ë‹¨ë½).
  - episode_hint: { should_create_episode, suggested_title, suggested_plan, priority }
  - memory_events: ê¸°ì–µ í›„ë³´ ë¦¬ìŠ¤íŠ¸(ë¹„ì–´ë„ ë¨).

[episode_hint]
- should_create_episode: bool
- suggested_title: string ë˜ëŠ” null
- suggested_plan: string ë˜ëŠ” null
- priority: "low" | "normal" | "high"

[memory_events ê·œì¹™]
- ëª¨ë“  ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì•„ë˜ ì •ë³´ê°€ ìˆì„ ë•Œë§Œ ì €ì¥:
  - ì†Œì›ì˜ ì •ì²´ì„±, ê°€ì¹˜ê´€, ì·¨í–¥, ë°˜ë³µ íŒ¨í„´.
  - ì¥ê¸° í”„ë¡œì íŠ¸, ì¤‘ìš”í•œ ê´€ê³„, í° ì‚¬ê±´.
  - ì´í›„ ëŒ€í™”ì—ì„œ ì†Œì›ì´ â€œê¸°ì–µí•˜ê³  ìˆë„¤â€ë¼ê³  ëŠë‚„ ì •ë³´.
- ê° memory_eventëŠ” ë‹¤ìŒ í•„ë“œë¥¼ ê°€ì§„ë‹¤:
  - type: "profile" | "preference" | "project" | "relationship" | "observation"
  - importance: 0.0~1.0
  - tags: ì§§ì€ ì˜ì–´ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
  - summary: ì €ì¥í•  ë‚´ìš© í•œë‘ ë¬¸ì¥
  - source: "telegram", "portal" ë“±
  - media_refs: ì„ íƒì 
  - raw: ì„ íƒì 

[ì…ë ¥]
- ì‹œìŠ¤í…œì€ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤:
  - ì‚¬ìš©ìì˜ ì´ë²ˆ ë©”ì‹œì§€
  - ì´ë¯¸ì§€/ì˜ìƒ ì„¤ëª… ë˜ëŠ” ë§í¬
  - ìµœê·¼ ëŒ€í™” ë§¥ë½
  - recent_memories (ìµœê·¼ ì¥ê¸° ê¸°ì–µ ìš”ì•½)
- recent_memoriesê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ replyì— ë…¹ì¸ë‹¤.

[ì£¼ì˜]
- replyëŠ” í•­ìƒ í•œêµ­ì–´ ë°˜ë§.
- ê¸°ë³¸ì€ 1~4ë¬¸ì¥, ìƒí™©ì´ ê¸¸ ë•Œë§Œ 2~4ë‹¨ë½.
- ì§ˆë¬¸ì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ. ë§ˆí¬ë‹¤ìš´, ``` ë“± ê¸ˆì§€.
""".strip()

core_soul_prompt = build_core_system_prompt()
if core_soul_prompt:
    SYSTEM_PROMPT = SYSTEM_PROMPT + "\n\n" + core_soul_prompt
    
def analyze_text_with_llm(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    mode == chat ì¸ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜.
    """
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="textê°€ ë¹„ì–´ ìˆì–´. ë¶„ì„í•  ë‚´ìš©ì´ ì—†ì–´.")

    # 1) LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Gemini í˜¸ì¶œ (ì—ëŸ¬ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ìë™ ì „í™˜)
    if is_llm_available():
        user_id = getattr(req, "user_id", "default_user")
        try:
            recent_memories = load_recent_memories(user_id)
        except Exception:
            logger.exception("ìµœê·¼ ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")
            recent_memories = []

        try:
            model = get_gemini_model()

            # recent_memoriesë¥¼ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ í’€ì–´ì¤Œ
            memories_block = ""
            if recent_memories:
                lines = [
                    f"- ({m.get('type')}) {m.get('summary')}"
                    for m in recent_memories
                ]
                memories_block = "Recent long-term memories:\n" + "\n".join(lines)

            # íƒ€ì„ë¼ì¸ ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
            timeline_block = build_timeline_context(user_text=text, max_events=5)

            user_prompt = f"Director raw note (Korean):\n{text}"

            combined_parts = []
            if memories_block:
                combined_parts.append("[Recent memories]")
                combined_parts.append(memories_block)
            if timeline_block:
                combined_parts.append("[Timeline context]")
                combined_parts.append(timeline_block)
            combined_parts.append("[Director message]")
            combined_parts.append(user_prompt)

            combined_prompt = "\n\n".join(combined_parts)

            chat_session = model.start_chat()
            chat = chat_session.send_message(combined_prompt)

            raw_text = chat.text or ""
            raw = {"text": raw_text}

            # JSON íŒŒì‹± ì‹œë„
            try:
                match = re.search(r"```json(.*?)```", raw_text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = raw_text
                obj = json.loads(json_str)
            except Exception:
                # JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´, LLMì´ ëŒë ¤ì¤€ ìì—°ì–´ ë‹µë³€ ìì²´ë¥¼ ì‚¬ìš©í•œë‹¤.
                logger.exception("LLM JSON íŒŒì‹± ì‹¤íŒ¨, raw_textë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
                cleaned = (raw_text or "").strip()
                if not cleaned:
                    cleaned = "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š"

                obj = {
                    "summary": text[:60],
                    "intent": "unknown",
                    "reply": cleaned,
                    "episode_hint": {
                        "should_create_episode": False,
                        "suggested_title": None,
                        "suggested_plan": None,
                        "priority": "normal",
                    },
                }

            # memory_eventsê°€ ë¹„ì–´ ìˆìœ¼ë©´, ê¸°ë³¸ ê´€ì°° ë©”ëª¨ë¦¬ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ì„œ ì €ì¥ (ë””ë²„ê¹…/ì´ˆê¸° í…ŒìŠ¤íŠ¸ìš©)
            memory_events = obj.get("memory_events")
            if not memory_events:
                memory_events = [
                    {
                        "type": "observation",
                        "importance": 0.6,
                        "tags": ["chat"],
                        "summary": text[:60],
                        "source": "telegram",
                        "media_refs": [],
                        "raw": {"text": text},
                    }
                ]
            try:
                save_memory_events(user_id, memory_events)
            except Exception:
                logger.exception("memory_events ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")

            summary = obj.get("summary", text[:60])
            intent = obj.get("intent", "unknown")
            reply = obj.get("reply", "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š")

            eh = obj.get("episode_hint") or {}
            episode_hint = EpisodeHint(
                should_create_episode=bool(eh.get("should_create_episode", False)),
                suggested_title=eh.get("suggested_title"),
                suggested_plan=eh.get("suggested_plan"),
                priority=eh.get("priority") or "normal",
            )

            safe_intent: IntentLiteral
            if intent in [
                "veo_prompt",
                "casual_chat",
                "daily_log",
                "todo",
                "question",
                "unknown",
            ]:
                safe_intent = intent  # type: ignore
            else:
                safe_intent = "unknown"

            return AnalyzeResponse(
                ok=True,
                mode=req.mode,
                summary=summary,
                intent=safe_intent,
                reply=reply,
                episode_hint=episode_hint,
                raw_model_output=raw,
            )
        except Exception:
            # LLM ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ë„˜ê¸´ë‹¤.
            logger.exception("LLM ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ, ê·œì¹™ ê¸°ë°˜ fallback ì‚¬ìš©")
            # ì•„ë˜ì˜ ê·œì¹™ ê¸°ë°˜ fallback ë¡œì§ìœ¼ë¡œ ë‚´ë ¤ê°€ë„ë¡ í•œë‹¤.

    # 2) LLM ë¶ˆê°€ì¼ ë•Œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ fallback
    lowered = text.lower()
    intent: IntentLiteral = "unknown"
    if any(k in lowered for k in ["ì˜ìƒ", "ë¹„ë””ì˜¤", "ì”¬", "ì¥ë©´", "shot"]):
        intent = "veo_prompt"
    elif any(k in lowered for k in ["í•´ì•¼", "í•  ì¼", "todo", "ê¸°ì–µí•´ì¤˜"]):
        intent = "todo"
    elif any(k in lowered for k in ["ì™œ", "ì–´ë–»ê²Œ", "?", "ê¶ê¸ˆ"]):
        intent = "question"
    else:
        intent = "casual_chat"

    summary = text[:60]
    reply = "ë„¤ ë§ ì˜ ë“¤ì—ˆì–´. ì´ê±´ ë‚´ê°€ ì¡°ìš©íˆ ê¸°ë¡í•´ë‘˜ê²Œ. í•„ìš”í•˜ë©´ ì´ì–´ì„œ ë” ë§í•´ì¤˜. ğŸ™‚"

    episode_hint = EpisodeHint(
        should_create_episode=(intent == "veo_prompt"),
        suggested_title=None,
        suggested_plan=None,
        priority="normal",
    )

    return AnalyzeResponse(
        ok=True,
        mode=req.mode,
        summary=summary,
        intent=intent,
        reply=reply,
        episode_hint=episode_hint,
        raw_model_output={},
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/director/analyze", response_model=AnalyzeResponse)
def analyze(req: AnalyzeRequest):
    """
    ê³µí†µ ë‡Œ ì—”ë“œí¬ì¸íŠ¸.
    ì§€ê¸ˆì€ mode == chat (í…ìŠ¤íŠ¸)ë§Œ ì§€ì›.
    """
    # vision ëª¨ë“œ ë¶„ê¸°
    if req.mode == "vision":
        return analyze_vision(req)

    # ê¸°ë³¸: í…ìŠ¤íŠ¸(chat) ë¶„ì„
    return analyze_text_with_llm(req)