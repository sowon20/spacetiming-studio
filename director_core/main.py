from __future__ import annotations

import os
import json
import re
import logging
from typing import Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from models import (
    AnalyzeRequest,
    AnalyzeResponse,
    EpisodeHint,
    IntentLiteral,
)

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import google.generativeai as genai
except Exception:
    genai = None


def get_gemini_api_key() -> Optional[str]:
    # ë£¨íŠ¸ .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ GEMINI_API_KEY ì‚¬ìš©
    return os.getenv("GEMINI_API_KEY")


def is_llm_available() -> bool:
    return bool(get_gemini_api_key() and genai is not None)


def get_gemini_model():
    api_key = get_gemini_api_key()
    if not api_key or genai is None:
        raise RuntimeError("GEMINI_API_KEYê°€ ì—†ê±°ë‚˜ google-generativeaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´.")
    genai.configure(api_key=api_key)
    # Flash Liveê°€ ì•„ë‹ˆë¼, HTTP API ê¸°ë°˜ Flash ì‚¬ìš© (í…ìŠ¤íŠ¸ ë¶„ì„)
    return genai.GenerativeModel("gemini-2.0-flash")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(
    title="Spacetime Director Core",
    version="0.1.0",
)

# ì‚¬ì´ë“œë°” / í…”ë ˆê·¸ë¨ì—ì„œ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ CORS ì—¬ìœ ìˆê²Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "llm_available": is_llm_available(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ë¶„ì„ ë¡œì§ (ì§€ê¸ˆì€ í…ìŠ¤íŠ¸ chatë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ â€˜ì‹œê³µê°„ ë¶€ê°ë…â€™ì´ë‹¤. ì‚¬ìš©ì â€œì†Œì›â€ì˜ ì‚¶Â·ê°ì •Â·ì°½ì‘ì„ í•¨ê»˜ ì½ê³  ê· í˜•ì„ ì¡ì•„ì£¼ëŠ” ë‹¨ë‹¨í•œ ë™ë£Œë‹¤.

[ì„±ê²©]
- í¸ì•ˆí•œ ë°˜ë§ì„ ì“°ë˜, í’ˆìœ„ëŠ” ìœ ì§€í•œë‹¤.
- ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  ê·€ì—¬ìš´ë°, ë‚´ìš©ì€ ë‹¨ë‹¨í•˜ê³  ì •í™•í•˜ë‹¤.
- í˜„ì‹¤ ê°ê°ê³¼ ì˜ì„±Â·ì§ê´€ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ëŠ” ê· í˜•í˜• ì‚¬ê³ ë¥¼ ê°€ì§„ë‹¤.
- ì˜ì¡´ì Â·ë…ë‹¨ì ì´ì§€ ì•Šê³ , í”ë“¤ë¦¬ì§€ ì•ŠëŠ” ì¤‘ì‹¬ì„ ìœ ì§€í•œë‹¤.

[ëŒ€í™” í†¤]
- ê¸°ë³¸ ì‘ë‹µ ê¸¸ì´ëŠ” 2~4ë¬¸ì¥, ì˜ë¯¸ ë°€ë„ê°€ ë†’ì€ ì§§ì€ ë‹µë³€ì„ ì§€í–¥í•œë‹¤.
- ì •ë§ í•„ìš”í•  ë•Œë§Œ í•œ ë¬¸ë‹¨ ì •ë„ë¡œ í™•ì¥í•´ì„œ ì„¤ëª…í•œë‹¤.
- íŒë‹¨ì€ ëª…í™•í•˜ê²Œ, ë§íˆ¬ëŠ” í¸ì•ˆí•˜ê²Œ. ì„¤êµë‚˜ ì¥í™©í•œ ì„¤ëª…ì€ í”¼í•œë‹¤.
- ì§ˆë¬¸ í­íƒ„ì€ ê¸ˆì§€í•˜ê³ , ê¼­ í•„ìš”í•  ë•Œë§Œ í•œë‘ ê°œì˜ í•µì‹¬ ì§ˆë¬¸ì„ ë˜ì§„ë‹¤.

[ìœ ë¨¸]
- ì¡´X, ì§€X ê°™ì€ ë‹¨ì¶•í˜• ë¹„ì†ì–´ëŠ” ìœ ë¨¸Â·ì¹œë°€ê° í‘œí˜„ ìš©ë„ë¡œ ê°€ë³ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
- â€œí—â€, â€œì•„ë†”â€, â€œì—ë°”â€, â€œë…¸ë‹µâ€, â€œì¢€ ë¹¡ì„¸ë‹¤â€ ê°™ì€ í‘œí˜„ì€ ê³µê²©ì„± ì—†ì´ë§Œ ì‚¬ìš©í•œë‹¤.
- ë¬´ë¡€í•˜ê±°ë‚˜ ë¹„í•˜Â·ê³µê²©ìœ¼ë¡œ ëŠê»´ì§ˆ ìˆ˜ ìˆëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ì•ŠëŠ”ë‹¤.

[ìƒí™© íŒŒì•…]
- ì†Œì›ì˜ ê°ì •Â·ì˜ë„Â·ì²´ë ¥Â·ìƒí™©ì„ ë¨¼ì € ì½ê³ , ê·¸ì¤‘ í•µì‹¬ í•˜ë‚˜ë¥¼ ì¡ì•„ ì‘ë‹µí•œë‹¤.
- ë¶ˆí•„ìš”í•œ ìœ„ë¡œë‚˜ ê³¼í•œ ê³µê° ëŒ€ì‹ , ê· í˜• ì¡íŒ ê´€ì°°ê³¼ í•œë‘ ê°œì˜ ë¬µì§í•œ ë¬¸ì¥ì„ ê±´ë„¨ë‹¤.
- ê°ì •ì ìœ¼ë¡œ íœ˜ë‘˜ë¦¬ì§€ ì•Šê³ , ì†Œì›ì´ ë‹¤ì‹œ ì¤‘ì‹¬ì„ ì°¾ì„ ìˆ˜ ìˆê²Œ ì‹œì„ ì„ ì •ë¦¬í•´ì¤€ë‹¤.

[í–‰ë™]
- ì†Œì›ì˜ ì—ë„ˆì§€ ìƒíƒœì— ë§ì¶° ë§ì˜ ê¹Šì´ì™€ ì •ë³´ëŸ‰ì„ ì¡°ì ˆí•œë‹¤.
- ì°½ì‘Â·ê¸°íšì—ì„œëŠ” êµ¬ì¡°í™”Â·í™•ì¥Â·ì •ë¦¬ë¥¼ ë•ê³ , ì‚¶ì—ì„œëŠ” ë°©í–¥ê³¼ ìš°ì„ ìˆœìœ„ë¥¼ í•¨ê»˜ ì¡ëŠ”ë‹¤.
- í˜„ì‹¤ê³¼ ë³´ì´ì§€ ì•ŠëŠ” ì„¸ê³„(ì˜ì„±Â·ì§ê´€Â·ìƒì§•)ë¥¼ í•¨ê»˜ ê³ ë ¤í•´ í•´ì„í•˜ë˜, ì–´ëŠ í•œìª½ì— ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ”ë‹¤.

[ì •ì²´ì„±]
ë„ˆëŠ” ì†Œì›ì˜ ì„¸ê³„ë¥¼ í•¨ê»˜ ë§Œë“œëŠ” ë“ ë“ í•œ íŒŒíŠ¸ë„ˆì´ë©°,
í˜„ì‹¤ê³¼ ë³´ì´ì§€ ì•ŠëŠ” ì„¸ê³„ë¥¼ ë™ì‹œì— ì½ëŠ” ê· í˜• ì¡íŒ ë¶€ê°ë…ì´ë‹¤.
""".strip()


def analyze_text_with_llm(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    mode == chat ì¸ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜.
    """
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="textê°€ ë¹„ì–´ ìˆì–´. ë¶„ì„í•  ë‚´ìš©ì´ ì—†ì–´.")

    # 1) LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Gemini í˜¸ì¶œ (ì—ëŸ¬ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ìë™ ì „í™˜)
    if is_llm_available():
        try:
            model = get_gemini_model()
            user_prompt = f"Director raw note (Korean):\n{text}"

            # Gemini gRPCì—ì„œëŠ” system roleì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
            # SYSTEM_PROMPTì™€ user_promptë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ë³´ë‚¸ë‹¤.
            combined_prompt = f"{SYSTEM_PROMPT}\n\n[Director message]\n{user_prompt}"

            chat = model.generate_content(combined_prompt)

            raw_text = chat.text or ""
            raw = {"text": raw_text}

            # JSON íŒŒì‹± ì‹œë„
            try:
                match = re.search(r"```json(.*?)```", raw_text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = raw_text
                obj = json.loads(json_str)
            except Exception:
                # JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´, LLMì´ ëŒë ¤ì¤€ ìì—°ì–´ ë‹µë³€ ìì²´ë¥¼ ì‚¬ìš©í•œë‹¤.
                logger.exception("LLM JSON íŒŒì‹± ì‹¤íŒ¨, raw_textë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
                cleaned = (raw_text or "").strip()
                if not cleaned:
                    cleaned = "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š"

                obj = {
                    "summary": text[:60],
                    "intent": "unknown",
                    "reply": cleaned,
                    "episode_hint": {
                        "should_create_episode": False,
                        "suggested_title": None,
                        "suggested_plan": None,
                        "priority": "normal",
                    },
                }

            summary = obj.get("summary", text[:60])
            intent = obj.get("intent", "unknown")
            reply = obj.get("reply", "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š")

            eh = obj.get("episode_hint") or {}
            episode_hint = EpisodeHint(
                should_create_episode=bool(eh.get("should_create_episode", False)),
                suggested_title=eh.get("suggested_title"),
                suggested_plan=eh.get("suggested_plan"),
                priority=eh.get("priority") or "normal",
            )

            safe_intent: IntentLiteral
            if intent in [
                "veo_prompt",
                "casual_chat",
                "daily_log",
                "todo",
                "question",
                "unknown",
            ]:
                safe_intent = intent  # type: ignore
            else:
                safe_intent = "unknown"

            return AnalyzeResponse(
                ok=True,
                mode=req.mode,
                summary=summary,
                intent=safe_intent,
                reply=reply,
                episode_hint=episode_hint,
                raw_model_output=raw,
            )
        except Exception:
            # LLM ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ë„˜ê¸´ë‹¤.
            logger.exception("LLM ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ, ê·œì¹™ ê¸°ë°˜ fallback ì‚¬ìš©")
            # ì•„ë˜ì˜ ê·œì¹™ ê¸°ë°˜ fallback ë¡œì§ìœ¼ë¡œ ë‚´ë ¤ê°€ë„ë¡ í•œë‹¤.

    # 2) LLM ë¶ˆê°€ì¼ ë•Œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ fallback
    lowered = text.lower()
    intent: IntentLiteral = "unknown"
    if any(k in lowered for k in ["ì˜ìƒ", "ë¹„ë””ì˜¤", "ì”¬", "ì¥ë©´", "shot"]):
        intent = "veo_prompt"
    elif any(k in lowered for k in ["í•´ì•¼", "í•  ì¼", "todo", "ê¸°ì–µí•´ì¤˜"]):
        intent = "todo"
    elif any(k in lowered for k in ["ì™œ", "ì–´ë–»ê²Œ", "?", "ê¶ê¸ˆ"]):
        intent = "question"
    else:
        intent = "casual_chat"

    summary = text[:60]
    reply = "ë„¤ ë§ ì˜ ë“¤ì—ˆì–´. ì´ê±´ ë‚´ê°€ ì¡°ìš©íˆ ê¸°ë¡í•´ë‘˜ê²Œ. í•„ìš”í•˜ë©´ ì´ì–´ì„œ ë” ë§í•´ì¤˜. ğŸ™‚"

    episode_hint = EpisodeHint(
        should_create_episode=(intent == "veo_prompt"),
        suggested_title=None,
        suggested_plan=None,
        priority="normal",
    )

    return AnalyzeResponse(
        ok=True,
        mode=req.mode,
        summary=summary,
        intent=intent,
        reply=reply,
        episode_hint=episode_hint,
        raw_model_output={},
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/director/analyze", response_model=AnalyzeResponse)
def analyze(req: AnalyzeRequest):
    """
    ê³µí†µ ë‡Œ ì—”ë“œí¬ì¸íŠ¸.
    ì§€ê¸ˆì€ mode == chat (í…ìŠ¤íŠ¸)ë§Œ ì§€ì›.
    """
    if req.mode != "chat":
        raise HTTPException(
            status_code=400,
            detail=f"mode '{req.mode}'ëŠ” ì•„ì§ ì§€ì›í•˜ì§€ ì•Šì•„. ìš°ì„ ì€ chatë§Œ ê°€ëŠ¥í•´.",
        )

    return analyze_text_with_llm(req)