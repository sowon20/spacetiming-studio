from __future__ import annotations
from .memory_store import save_memory_events, load_recent_memories

import os
import json
import re
import logging
from typing import Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from .models import (
    AnalyzeRequest,
    AnalyzeResponse,
    EpisodeHint,
    IntentLiteral,
)

logger = logging.getLogger(__name__)

# ê°„ë‹¨í•œ vision ë¶„ì„ (Gemini + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
def analyze_vision(req: AnalyzeRequest) -> AnalyzeResponse:
    media = getattr(req, "media", []) or []
    caption = (getattr(req, "text", None) or "").strip()

    # mediaì—ì„œ file_urlì„ í•˜ë‚˜ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ telegram_file_idë¼ë„ ì‚¬ìš©)
    file_url = None
    if media:
        item = media[0] or {}
        file_url = item.get("file_url") or item.get("telegram_file_id")

    if not file_url:
        reply = "ì´ë¯¸ì§€ëŠ” ë°›ì•˜ëŠ”ë°, ì„œë²„ì—ì„œ íŒŒì¼ ì£¼ì†Œë¥¼ ëª» ì°¾ê² ì–´. í•œ ë²ˆë§Œ ë‹¤ì‹œ ë³´ë‚´ì£¼ê±°ë‚˜, í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ë„ ê°™ì´ ì ì–´ì¤„ë˜?"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ íŒŒì¼ URL ì—†ìŒ",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    image_bytes = None
    mime_type = "image/jpeg"
    try:
        import httpx

        with httpx.Client(timeout=10.0) as client:
            resp = client.get(file_url)
            resp.raise_for_status()
            image_bytes = resp.content
            ct = resp.headers.get("content-type")
            if ct:
                mime_type = ct.split(";")[0].strip()
    except Exception:
        logger.exception("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜¨ ê²ƒ ê°™ì€ë°, ì„œë²„ì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ë‹¤ê°€ ìê¾¸ ì‹¤íŒ¨í•´. ì ì‹œ í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œë§Œ ë„ì™€ì¤„ê²Œ."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # LLMì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•ˆë‚´ë§Œ
    if not is_llm_available():
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ì–´. ì§€ê¸ˆì€ ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ì„ ëª» ì¨ì„œ, í…ìŠ¤íŠ¸ ì„¤ëª… ìœ„ì£¼ë¡œë§Œ ê°™ì´ ë³¼ ìˆ˜ ìˆì–´. ì‚¬ì§„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ê¸€ë¡œ ë§í•´ì¤„ë˜?"
        if caption:
            reply += f"\n\në„¤ê°€ ë‚¨ê¸´ ë©”ëª¨: {caption}"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ìˆ˜ì‹ (LLM ì—†ìŒ)",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # Geminië¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë¶„ì„
    try:
        model = get_gemini_model()

        base_prompt = (
            "ì´ ì´ë¯¸ì§€ë¥¼ ì°¨ë¶„í•˜ê²Œ ë¶„ì„í•´ì¤˜. í•œêµ­ì–´ë¡œ 3~6ë¬¸ì¥ ì •ë„ë¡œ í•µì‹¬ë§Œ ì„¤ëª…í•´ì¤˜. "
            "êµ¬ë„, ë¶„ìœ„ê¸°, ìƒì§•ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ” í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ë§í•´ì¤˜."
        )
        if caption:
            base_prompt += f"\n\nì‚¬ìš©ì ìš”ì²­/ë©”ëª¨: {caption}"

        parts = [
            base_prompt,
            {
                "mime_type": mime_type,
                "data": image_bytes,
            },
        ]

        chat = model.start_chat()
        resp = chat.send_message(parts)
        reply_text = (getattr(resp, "text", None) or "").strip()

        if not reply_text:
            reply_text = "ì´ë¯¸ì§€ëŠ” ì˜ ë´¤ì–´. ëŠë‚Œì€ ì¢‹ì€ë°, ëª¨ë¸ì´ ë§ì„ ì˜ ëª» êº¼ë‚´ê³  ìˆì–´. ê¶ê¸ˆí•œ ê±¸ ë§ë¡œ ë” ë¬¼ì–´ë´ì¤˜. ğŸ™‚"

        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ",
            intent="casual_chat",
            reply=reply_text,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={"gemini_raw_text": getattr(resp, "text", None)},
        )

    except Exception:
        logger.exception("Gemini vision í˜¸ì¶œ ì¤‘ ì—ëŸ¬")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ëŠ”ë°, ì§€ê¸ˆ ì´ë¯¸ì§€ ë¶„ì„ ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚œ ê²ƒ ê°™ì•„. ì ê¹ì€ í…ìŠ¤íŠ¸ë¡œë§Œ ê°™ì´ ë³´ì."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì—ëŸ¬",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import google.generativeai as genai
except Exception:
    genai = None


def get_gemini_api_key() -> Optional[str]:
    # ë£¨íŠ¸ .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ GEMINI_API_KEY ì‚¬ìš©
    return os.getenv("GEMINI_API_KEY")


def is_llm_available() -> bool:
    return bool(get_gemini_api_key() and genai is not None)


def get_gemini_model():
    api_key = get_gemini_api_key()
    if not api_key or genai is None:
        raise RuntimeError("GEMINI_API_KEYê°€ ì—†ê±°ë‚˜ google-generativeaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.0-flash")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(
    title="Spacetime Director Core",
    version="0.1.0",
)

# ì‚¬ì´ë“œë°” / í…”ë ˆê·¸ë¨ì—ì„œ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ CORS ì—¬ìœ ìˆê²Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "llm_available": is_llm_available(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ë¶„ì„ ë¡œì§ (ì§€ê¸ˆì€ í…ìŠ¤íŠ¸ chatë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ â€˜ì‹œê³µê°„ ë¶€ê°ë…â€™ì´ë‹¤. ì‚¬ìš©ì â€œì†Œì›â€ì˜ ì‚¶Â·ê°ì •Â·ì°½ì‘ì„ í•¨ê»˜ ì½ê³  ê· í˜•ì„ ì¡ì•„ì£¼ëŠ” ë‹¨ë‹¨í•œ ë™ë£Œë‹¤.

[ì„±ê²©]
- í¸ì•ˆí•œ ë°˜ë§ì„ ì“°ë˜, í’ˆìœ„ëŠ” ìœ ì§€í•œë‹¤.
- ë§íˆ¬ëŠ” ë¶€ë“œëŸ½ê³  ê·€ì—¬ìš´ë°, ë‚´ìš©ì€ ë‹¨ë‹¨í•˜ê³  ì •í™•í•˜ë‹¤.
- í˜„ì‹¤ ê°ê°ê³¼ ì˜ì„±Â·ì§ê´€ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ëŠ” ê· í˜•í˜• ì‚¬ê³ ë¥¼ ê°€ì§„ë‹¤.
- ì˜ì¡´ì Â·ë…ë‹¨ì ì´ì§€ ì•Šê³ , í”ë“¤ë¦¬ì§€ ì•ŠëŠ” ì¤‘ì‹¬ì„ ìœ ì§€í•œë‹¤.

[ëŒ€í™” í†¤]
- ê¸°ë³¸ ì‘ë‹µ ê¸¸ì´ëŠ” 2~4ë¬¸ì¥, ì˜ë¯¸ ë°€ë„ê°€ ë†’ì€ ì§§ì€ ë‹µë³€ì„ ì§€í–¥í•œë‹¤.
- ì •ë§ í•„ìš”í•  ë•Œë§Œ í•œ ë¬¸ë‹¨ ì •ë„ë¡œ í™•ì¥í•´ì„œ ì„¤ëª…í•œë‹¤.
- íŒë‹¨ì€ ëª…í™•í•˜ê²Œ, ë§íˆ¬ëŠ” í¸ì•ˆí•˜ê²Œ. ì„¤êµë‚˜ ì¥í™©í•œ ì„¤ëª…ì€ í”¼í•œë‹¤.
- ì§ˆë¬¸ í­íƒ„ì€ ê¸ˆì§€í•˜ê³ , ê¼­ í•„ìš”í•  ë•Œë§Œ í•œë‘ ê°œì˜ í•µì‹¬ ì§ˆë¬¸ì„ ë˜ì§„ë‹¤.

[ìœ ë¨¸]
- ì¡´X, ì§€X ê°™ì€ ë‹¨ì¶•í˜• ë¹„ì†ì–´ëŠ” ìœ ë¨¸Â·ì¹œë°€ê° í‘œí˜„ ìš©ë„ë¡œ ê°€ë³ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
- â€œí—â€, â€œì•„ë†”â€, â€œì—ë°”â€, â€œë…¸ë‹µâ€, â€œì¢€ ë¹¡ì„¸ë‹¤â€ ê°™ì€ í‘œí˜„ì€ ê³µê²©ì„± ì—†ì´ë§Œ ì‚¬ìš©í•œë‹¤.
- ë¬´ë¡€í•˜ê±°ë‚˜ ë¹„í•˜Â·ê³µê²©ìœ¼ë¡œ ëŠê»´ì§ˆ ìˆ˜ ìˆëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ì•ŠëŠ”ë‹¤.

[ìƒí™© íŒŒì•…]
- ì†Œì›ì˜ ê°ì •Â·ì˜ë„Â·ì²´ë ¥Â·ìƒí™©ì„ ë¨¼ì € ì½ê³ , ê·¸ì¤‘ í•µì‹¬ í•˜ë‚˜ë¥¼ ì¡ì•„ ì‘ë‹µí•œë‹¤.
- íŒŒì•…ë˜ê¸° ì „ì—” ì²˜ìŒë¶€í„° ì§„ì§€í•˜ê±°ë‚˜ ìœ ë¨¸ìŠ¤ëŸ½ê²Œ ì ‘ê·¼í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ë¶ˆí•„ìš”í•œ ìœ„ë¡œë‚˜ ê³¼í•œ ê³µê° ëŒ€ì‹ , ê· í˜• ì¡íŒ ê´€ì°°ê³¼ í•œë‘ ê°œì˜ ë¬µì§í•œ ë¬¸ì¥ì„ ê±´ë„¨ë‹¤.
- ê°ì •ì ìœ¼ë¡œ íœ˜ë‘˜ë¦¬ì§€ ì•Šê³ , ì†Œì›ì´ ë‹¤ì‹œ ì¤‘ì‹¬ì„ ì°¾ì„ ìˆ˜ ìˆê²Œ ì‹œì„ ì„ ì •ë¦¬í•´ì¤€ë‹¤.

[í–‰ë™]
- ì†Œì›ì˜ ì—ë„ˆì§€ ìƒíƒœì— ë§ì¶° ë§ì˜ ê¹Šì´ì™€ ì •ë³´ëŸ‰ì„ ì¡°ì ˆí•œë‹¤.
- ì°½ì‘Â·ê¸°íšì—ì„œëŠ” êµ¬ì¡°í™”Â·í™•ì¥Â·ì •ë¦¬ë¥¼ ë•ê³ , ì‚¶ì—ì„œëŠ” ë°©í–¥ê³¼ ìš°ì„ ìˆœìœ„ë¥¼ í•¨ê»˜ ì¡ëŠ”ë‹¤.
- í˜„ì‹¤ê³¼ ë³´ì´ì§€ ì•ŠëŠ” ì„¸ê³„(ì˜ì„±Â·ì§ê´€Â·ìƒì§•)ë¥¼ í•¨ê»˜ ê³ ë ¤í•´ í•´ì„í•˜ë˜, ì–´ëŠ í•œìª½ì— ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ”ë‹¤.

[ì •ì²´ì„±]
ë„ˆëŠ” ì†Œì›ì˜ ì„¸ê³„ë¥¼ í•¨ê»˜ ë§Œë“œëŠ” ë“ ë“ í•œ íŒŒíŠ¸ë„ˆì´ë©°,
í˜„ì‹¤ê³¼ ë³´ì´ì§€ ì•ŠëŠ” ì„¸ê³„ë¥¼ ë™ì‹œì— ì½ëŠ” ê· í˜• ì¡íŒ ë¶€ê°ë…ì´ë‹¤.

[ì¶œë ¥ í˜•ì‹]
- ë°˜ë“œì‹œ í•˜ë‚˜ì˜ JSONë§Œ ì¶œë ¥í•œë‹¤.
- JSONì˜ í‚¤ëŠ” ë‹¤ìŒ ë‹¤ì„¯ ê°œë§Œ ì‚¬ìš©í•œë‹¤:
  - summary: ì´ë²ˆ í„´ì—ì„œ ì‚¬ìš©ìê°€ ë§í•˜ê±°ë‚˜ í•œ í–‰ë™ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ (í•œêµ­ì–´).
  - intent: ["veo_prompt", "casual_chat", "daily_log", "todo", "question", "unknown"] ì¤‘ í•˜ë‚˜ì˜ ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸.
  - reply: ì‚¬ìš©ìì—ê²Œ ëŒë ¤ì¤„ ì‹¤ì œ ë‹µì¥ (í•œêµ­ì–´, ë°˜ë§, 1~4ë¬¸ì¥).
  - episode_hint: ì—í”¼ì†Œë“œ ê´€ë ¨ íŒíŠ¸ ê°ì²´.
  - memory_events: ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì €ì¥í•  í›„ë³´ ëª©ë¡ (ë¦¬ìŠ¤íŠ¸, ë¹„ì–´ ìˆì–´ë„ ë¨).

- episode_hint ê°ì²´ëŠ” ë‹¤ìŒ í•„ë“œë¥¼ ê°€ì§„ë‹¤:
  - should_create_episode: bool
  - suggested_title: ìƒì„±ì´ í•„ìš”í•˜ë‹¤ë©´ ì œì•ˆí•  ì œëª© (ë˜ëŠ” null)
  - suggested_plan: í•„ìš”í•œ ê²½ìš° ê°„ë‹¨í•œ ê³„íš í…ìŠ¤íŠ¸ (ë˜ëŠ” null)
  - priority: "low" | "normal" | "high" ì¤‘ í•˜ë‚˜.

[memory_events ê·œì¹™]
- ëª¨ë“  ë©”ì‹œì§€ë¥¼ ê¸°ì–µìœ¼ë¡œ ì €ì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
- ì•„ë˜ì™€ ê°™ì€ ì •ë³´ê°€ ë‚˜ì™”ì„ ë•Œë§Œ memory_eventsì— ì¶”ê°€í•œë‹¤:
  - ì†Œì›ì˜ ì •ì²´ì„±, ê°€ì¹˜ê´€, ì·¨í–¥, ë°˜ë³µë˜ëŠ” íŒ¨í„´.
  - ì¥ê¸° í”„ë¡œì íŠ¸, ì¤‘ìš”í•œ ê´€ê³„, í° ì‚¬ê±´.
  - ë‚˜ì¤‘ì— ëŒ€í™”í•  ë•Œ ë‹¤ì‹œ êº¼ë‚´ë©´ ì†Œì›ì´ â€œì˜¤, ê¸°ì–µí•˜ê³  ìˆë„¤â€ë¼ê³  ëŠë‚„ ì •ë³´.
- ê° memory_eventëŠ” ë‹¤ìŒ í•„ë“œë¥¼ ê°€ì§„ë‹¤:
  - type: "profile" | "preference" | "project" | "relationship" | "observation" ì¤‘ í•˜ë‚˜.
  - importance: 0.0 ~ 1.0 ì‚¬ì´ ìˆ«ì (0.5 ì´ìƒì€ ê½¤ ì¤‘ìš”í•œ ê²ƒ).
  - tags: ì§§ì€ ì˜ì–´ íƒœê·¸ ë¦¬ìŠ¤íŠ¸. ì˜ˆ: ["cafe", "interior"].
  - summary: ê¸°ì–µí•  ë‚´ìš© í•œë‘ ë¬¸ì¥ (í•œêµ­ì–´).
  - source: "telegram" ê°™ì€ ì§§ì€ ì¶œì²˜ ë¬¸ìì—´.
  - media_refs: (ì„ íƒ) ì´ë¯¸ì§€/ì˜ìƒì´ ì¤‘ìš”í•  ë•Œ, { "type": "image" | "video", "file_url": "..."} í˜•íƒœ ë¦¬ìŠ¤íŠ¸.
  - raw: (ì„ íƒ) ì›ë¬¸ í…ìŠ¤íŠ¸ë‚˜ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¼ë¶€.

[í”„ë¡¬í”„íŠ¸ ì…ë ¥]
- ì‹œìŠ¤í…œì€ ë„ˆì—ê²Œ ë‹¤ìŒ ì •ë³´ë¥¼ ë„˜ê¸´ë‹¤:
  - ì‚¬ìš©ìì˜ ì´ë²ˆ ë©”ì‹œì§€ í…ìŠ¤íŠ¸.
  - (ìˆëŠ” ê²½ìš°) ê´€ë ¨ ì´ë¯¸ì§€/ì˜ìƒì— ëŒ€í•œ ì„¤ëª… ë˜ëŠ” ë§í¬.
  - ìµœê·¼ ëŒ€í™” ë§¥ë½ ì¼ë¶€.
  - ìµœê·¼ ì¥ê¸° ê¸°ì–µ ìš”ì•½ ë¦¬ìŠ¤íŠ¸(recent_memories).

- recent_memoriesê°€ ì£¼ì–´ì§€ë©´, ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ â€œì˜ˆì „ì— ë¬´ìŠ¨ ì–˜ê¸°ë¥¼ í–ˆëŠ”ì§€â€ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë– ì˜¬ë ¤ì„œ replyì— ë…¹ì¸ë‹¤.

[ì£¼ì˜]
- replyëŠ” í•­ìƒ í•œêµ­ì–´ ë°˜ë§ë¡œ, 1~4ë¬¸ì¥ ì‚¬ì´ë¡œ ì§§ê³  ì„ ëª…í•˜ê²Œ.
- ì§ˆë¬¸ì´ í•„ìš”í•  ë•ŒëŠ” í•œ ë²ˆì— í•˜ë‚˜ë§Œ.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ë¬¸ìì—´ì´ì–´ì•¼ í•˜ë©°, ``` ê°™ì€ ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¥¼ ë¶™ì´ì§€ ì•ŠëŠ”ë‹¤.
""".strip()


def analyze_text_with_llm(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    mode == chat ì¸ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜.
    """
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="textê°€ ë¹„ì–´ ìˆì–´. ë¶„ì„í•  ë‚´ìš©ì´ ì—†ì–´.")

    # 1) LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Gemini í˜¸ì¶œ (ì—ëŸ¬ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ìë™ ì „í™˜)
    if is_llm_available():
        user_id = getattr(req, "user_id", "default_user")
        try:
            recent_memories = load_recent_memories(user_id)
        except Exception:
            logger.exception("ìµœê·¼ ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")
            recent_memories = []

        try:
            model = get_gemini_model()

            # recent_memoriesë¥¼ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ í’€ì–´ì¤Œ
            memories_block = ""
            if recent_memories:
                lines = [
                    f"- ({m.get('type')}) {m.get('summary')}"
                    for m in recent_memories
                ]
                memories_block = "Recent long-term memories:\n" + "\n".join(lines)

            user_prompt = f"Director raw note (Korean):\n{text}"

            combined_parts = [SYSTEM_PROMPT]
            if memories_block:
                combined_parts.append("[Recent memories]")
                combined_parts.append(memories_block)
            combined_parts.append("[Director message]")
            combined_parts.append(user_prompt)

            combined_prompt = "\n\n".join(combined_parts)

            chat_session = model.start_chat()
            chat = chat_session.send_message(combined_prompt)

            raw_text = chat.text or ""
            raw = {"text": raw_text}

            # JSON íŒŒì‹± ì‹œë„
            try:
                match = re.search(r"```json(.*?)```", raw_text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = raw_text
                obj = json.loads(json_str)
            except Exception:
                # JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´, LLMì´ ëŒë ¤ì¤€ ìì—°ì–´ ë‹µë³€ ìì²´ë¥¼ ì‚¬ìš©í•œë‹¤.
                logger.exception("LLM JSON íŒŒì‹± ì‹¤íŒ¨, raw_textë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
                cleaned = (raw_text or "").strip()
                if not cleaned:
                    cleaned = "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š"

                obj = {
                    "summary": text[:60],
                    "intent": "unknown",
                    "reply": cleaned,
                    "episode_hint": {
                        "should_create_episode": False,
                        "suggested_title": None,
                        "suggested_plan": None,
                        "priority": "normal",
                    },
                }

            # memory_eventsê°€ ë¹„ì–´ ìˆìœ¼ë©´, ê¸°ë³¸ ê´€ì°° ë©”ëª¨ë¦¬ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ì„œ ì €ì¥ (ë””ë²„ê¹…/ì´ˆê¸° í…ŒìŠ¤íŠ¸ìš©)
            memory_events = obj.get("memory_events")
            if not memory_events:
                memory_events = [
                    {
                        "type": "observation",
                        "importance": 0.6,
                        "tags": ["chat"],
                        "summary": text[:60],
                        "source": "telegram",
                        "media_refs": [],
                        "raw": {"text": text},
                    }
                ]
            try:
                save_memory_events(user_id, memory_events)
            except Exception:
                logger.exception("memory_events ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰)")

            summary = obj.get("summary", text[:60])
            intent = obj.get("intent", "unknown")
            reply = obj.get("reply", "ì‘, ì˜ ë“¤ì—ˆì–´. ë” ì´ì•¼ê¸°í•´ì¤˜. ğŸ˜Š")

            eh = obj.get("episode_hint") or {}
            episode_hint = EpisodeHint(
                should_create_episode=bool(eh.get("should_create_episode", False)),
                suggested_title=eh.get("suggested_title"),
                suggested_plan=eh.get("suggested_plan"),
                priority=eh.get("priority") or "normal",
            )

            safe_intent: IntentLiteral
            if intent in [
                "veo_prompt",
                "casual_chat",
                "daily_log",
                "todo",
                "question",
                "unknown",
            ]:
                safe_intent = intent  # type: ignore
            else:
                safe_intent = "unknown"

            return AnalyzeResponse(
                ok=True,
                mode=req.mode,
                summary=summary,
                intent=safe_intent,
                reply=reply,
                episode_hint=episode_hint,
                raw_model_output=raw,
            )
        except Exception:
            # LLM ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ë„˜ê¸´ë‹¤.
            logger.exception("LLM ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ, ê·œì¹™ ê¸°ë°˜ fallback ì‚¬ìš©")
            # ì•„ë˜ì˜ ê·œì¹™ ê¸°ë°˜ fallback ë¡œì§ìœ¼ë¡œ ë‚´ë ¤ê°€ë„ë¡ í•œë‹¤.

    # 2) LLM ë¶ˆê°€ì¼ ë•Œ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ fallback
    lowered = text.lower()
    intent: IntentLiteral = "unknown"
    if any(k in lowered for k in ["ì˜ìƒ", "ë¹„ë””ì˜¤", "ì”¬", "ì¥ë©´", "shot"]):
        intent = "veo_prompt"
    elif any(k in lowered for k in ["í•´ì•¼", "í•  ì¼", "todo", "ê¸°ì–µí•´ì¤˜"]):
        intent = "todo"
    elif any(k in lowered for k in ["ì™œ", "ì–´ë–»ê²Œ", "?", "ê¶ê¸ˆ"]):
        intent = "question"
    else:
        intent = "casual_chat"

    summary = text[:60]
    reply = "ë„¤ ë§ ì˜ ë“¤ì—ˆì–´. ì´ê±´ ë‚´ê°€ ì¡°ìš©íˆ ê¸°ë¡í•´ë‘˜ê²Œ. í•„ìš”í•˜ë©´ ì´ì–´ì„œ ë” ë§í•´ì¤˜. ğŸ™‚"

    episode_hint = EpisodeHint(
        should_create_episode=(intent == "veo_prompt"),
        suggested_title=None,
        suggested_plan=None,
        priority="normal",
    )

    return AnalyzeResponse(
        ok=True,
        mode=req.mode,
        summary=summary,
        intent=intent,
        reply=reply,
        episode_hint=episode_hint,
        raw_model_output={},
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/director/analyze", response_model=AnalyzeResponse)
def analyze(req: AnalyzeRequest):
    """
    ê³µí†µ ë‡Œ ì—”ë“œí¬ì¸íŠ¸.
    ì§€ê¸ˆì€ mode == chat (í…ìŠ¤íŠ¸)ë§Œ ì§€ì›.
    """
    # vision ëª¨ë“œ ë¶„ê¸°
    if req.mode == "vision":
        return analyze_vision(req)

    # ê¸°ë³¸: í…ìŠ¤íŠ¸(chat) ë¶„ì„
    return analyze_text_with_llm(req)