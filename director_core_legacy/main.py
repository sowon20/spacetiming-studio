from __future__ import annotations
from .memory_store import save_memory_events, load_recent_memories
from .imported_memory_loader import load_imported_memories
from .soul_loader import (
    build_core_system_prompt,
    build_timeline_context,
)
from .prompt_loader import load_base_system_prompt
from .prompt_assembler import assemble_prompt_for_llm

import os
import json
import re
import logging
from typing import Optional
from pathlib import Path

from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from director_core.config_manager import apply_config_updates

from .models import (
    AnalyzeRequest,
    AnalyzeResponse,
    EpisodeHint,
    IntentLiteral,
)

logger = logging.getLogger(__name__)

# ê°„ë‹¨í•œ vision ë¶„ì„ (Gemini + ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
def analyze_vision(req: AnalyzeRequest) -> AnalyzeResponse:
    media = getattr(req, "media", []) or []
    caption = (getattr(req, "text", None) or "").strip()

    # mediaì—ì„œ file_urlì„ í•˜ë‚˜ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ telegram_file_idë¼ë„ ì‚¬ìš©)
    file_url = None
    if media:
        item = media[0] or {}
        file_url = item.get("file_url") or item.get("telegram_file_id")

    if not file_url:
        reply = "ì´ë¯¸ì§€ëŠ” ë°›ì•˜ëŠ”ë°, ì„œë²„ì—ì„œ íŒŒì¼ ì£¼ì†Œë¥¼ ëª» ì°¾ê² ì–´. í•œ ë²ˆë§Œ ë‹¤ì‹œ ë³´ë‚´ì£¼ê±°ë‚˜, í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…ë„ ê°™ì´ ì ì–´ì¤„ë˜?"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ íŒŒì¼ URL ì—†ìŒ",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    image_bytes = None
    mime_type = "image/jpeg"
    try:
        import httpx

        with httpx.Client(timeout=10.0) as client:
            resp = client.get(file_url)
            resp.raise_for_status()
            image_bytes = resp.content
            ct = resp.headers.get("content-type")
            if ct:
                mime_type = ct.split(";")[0].strip()
    except Exception:
        logger.exception("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜¨ ê²ƒ ê°™ì€ë°, ì„œë²„ì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ë‹¤ê°€ ìê¾¸ ì‹¤íŒ¨í•´. ì ì‹œ í…ìŠ¤íŠ¸ ìœ„ì£¼ë¡œë§Œ ë„ì™€ì¤„ê²Œ."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # LLMì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì•ˆë‚´ë§Œ
    if not is_llm_available():
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ì–´. ì§€ê¸ˆì€ ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ì„ ëª» ì¨ì„œ, í…ìŠ¤íŠ¸ ì„¤ëª… ìœ„ì£¼ë¡œë§Œ ê°™ì´ ë³¼ ìˆ˜ ìˆì–´. ì‚¬ì§„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ê¸€ë¡œ ë§í•´ì¤„ë˜?"
        if caption:
            reply += f"\n\në„¤ê°€ ë‚¨ê¸´ ë©”ëª¨: {caption}"
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ìˆ˜ì‹ (LLM ì—†ìŒ)",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

    # Geminië¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë¶„ì„
    try:
        model = get_gemini_model()

        base_prompt = (
            "ì´ ì´ë¯¸ì§€ë¥¼ ì°¨ë¶„í•˜ê²Œ ë¶„ì„í•´ì¤˜. í•œêµ­ì–´ë¡œ 3~6ë¬¸ì¥ ì •ë„ë¡œ í•µì‹¬ë§Œ ì„¤ëª…í•´ì¤˜. "
            "êµ¬ë„, ë¶„ìœ„ê¸°, ìƒì§•ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ” í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ë§í•´ì¤˜."
        )
        if caption:
            base_prompt += f"\n\nì‚¬ìš©ì ìš”ì²­/ë©”ëª¨: {caption}"

        parts = [
            base_prompt,
            {
                "mime_type": mime_type,
                "data": image_bytes,
            },
        ]

        chat = model.start_chat()
        resp = chat.send_message(parts)
        reply_text = (getattr(resp, "text", None) or "").strip()

        if not reply_text:
            reply_text = "ì´ë¯¸ì§€ëŠ” ì˜ ë´¤ì–´. ëŠë‚Œì€ ì¢‹ì€ë°, ëª¨ë¸ì´ ë§ì„ ì˜ ëª» êº¼ë‚´ê³  ìˆì–´. ê¶ê¸ˆí•œ ê±¸ ë§ë¡œ ë” ë¬¼ì–´ë´ì¤˜. ğŸ™‚"

        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ",
            intent="casual_chat",
            reply=reply_text,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={"gemini_raw_text": getattr(resp, "text", None)},
        )

    except Exception:
        logger.exception("Gemini vision í˜¸ì¶œ ì¤‘ ì—ëŸ¬")
        reply = "ì´ë¯¸ì§€ëŠ” ì˜ ë°›ì•˜ëŠ”ë°, ì§€ê¸ˆ ì´ë¯¸ì§€ ë¶„ì„ ìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚œ ê²ƒ ê°™ì•„. ì ê¹ì€ í…ìŠ¤íŠ¸ë¡œë§Œ ê°™ì´ ë³´ì."
        return AnalyzeResponse(
            ok=True,
            mode="vision",
            summary="ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì—ëŸ¬",
            intent="casual_chat",
            reply=reply,
            episode_hint=EpisodeHint(
                should_create_episode=False,
                suggested_title=None,
                suggested_plan=None,
                priority="normal",
            ),
            raw_model_output={},
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import google.generativeai as genai
except Exception:
    genai = None


def get_gemini_api_key() -> Optional[str]:
    # ë£¨íŠ¸ .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ GEMINI_API_KEY ì‚¬ìš©
    return os.getenv("GEMINI_API_KEY")


def is_llm_available() -> bool:
    return bool(get_gemini_api_key() and genai is not None)


def get_gemini_model():
    api_key = get_gemini_api_key()
    if not api_key or genai is None:
        raise RuntimeError("GEMINI_API_KEYê°€ ì—†ê±°ë‚˜ google-generativeaië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´.")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.0-flash", system_instruction=SYSTEM_PROMPT)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FastAPI ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI(
    title="Spacetime Director Core",
    version="0.1.0",
)

# ì‚¬ì´ë“œë°” / í…”ë ˆê·¸ë¨ì—ì„œ í˜¸ì¶œí•  ìˆ˜ë„ ìˆìœ¼ë‹ˆ CORS ì—¬ìœ ìˆê²Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "llm_available": is_llm_available(),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚´ë¶€ ë¶„ì„ ë¡œì§ (ì§€ê¸ˆì€ í…ìŠ¤íŠ¸ chatë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = load_base_system_prompt()

core_soul_prompt = build_core_system_prompt()
if core_soul_prompt:
    SYSTEM_PROMPT = SYSTEM_PROMPT + "\n\n" + core_soul_prompt


def load_recent_dialogue(user_id: str, limit: int = 40) -> str:
    """
    portal_history/{user_id}.chat.jsonl ì—ì„œ ìµœê·¼ ëŒ€í™” ëª‡ ì¤„ì„ ì½ì–´ì„œ
    LLM í”„ë¡¬í”„íŠ¸ì— ë¶™ì¼ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë§Œë“ ë‹¤.
      ì˜ˆì‹œ:
        {"role": "user", "content": "..."}
        {"role": "assistant", "content": "..."}
    """
    try:
        history_path = Path("portal_history") / f"{user_id}.chat.jsonl"
        if not history_path.exists():
            return ""

        lines = history_path.read_text(encoding="utf-8").splitlines()
        if not lines:
            return ""

        # ğŸ”¹ ìµœê·¼ limitê°œë§Œ ì‚¬ìš© (í—›ì†Œë¦¬/ê°íƒ„ì‚¬ ì„ì—¬ë„ ê¸¸ì´ë¡œ ë°€ì–´ë¶™ì´ê¸°)
        recent = lines[-limit:]

        blocks: list[str] = []
        for line in recent:
            try:
                obj = json.loads(line)
            except Exception:
                continue

            role = (obj.get("role") or obj.get("speaker") or "user").strip()
            content = (obj.get("content") or obj.get("text") or "").strip()
            if not content:
                continue

            prefix = "assistant" if role == "assistant" else "user"
            blocks.append(f"{prefix}: {content}")

        if not blocks:
            return ""

        return "[Recent dialogue]\n" + "\n".join(blocks)

    except Exception:
        logger.exception("ìµœê·¼ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ (ë¬´ì‹œí•˜ê³  ì§„í–‰)")
        return ""

def rule_based_analyze(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    LLMì´ ì—†ê±°ë‚˜ ì—ëŸ¬ ë‚  ë•Œ ì“°ëŠ” ì•„ì£¼ ë‹¨ìˆœí•œ fallback.
    ì§€ê¸ˆì€ ê·¸ëƒ¥ ì›ë˜ í…ìŠ¤íŠ¸ë¥¼ ë˜ë¹„ì¶°ì£¼ëŠ” ì •ë„ë¡œë§Œ ë™ì‘í•˜ê²Œ ë‘”ë‹¤.
    """
    text = (req.text or "").strip()
    if not text:
        text = ""

    reply = (
        "ì§€ê¸ˆì€ LLM ìª½ì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ì„œ, ë³µì¡í•œ ë¶„ì„ ëŒ€ì‹  ì•„ì£¼ ë‹¨ìˆœí•˜ê²Œë§Œ ê°™ì´ ë³¼ ìˆ˜ ìˆì–´.\n"
        "ë„¤ê°€ ë°©ê¸ˆ ë³´ë‚¸ ë‚´ìš©ì€ ëŒ€ëµ ì´ëŸ° ëŠë‚Œì´ì•¼:\n\n"
        f"{text}"
    )

    return AnalyzeResponse(
        ok=True,
        mode=req.mode,
        summary=(text[:80] + "...") if len(text) > 80 else text,
        intent="casual_chat",
        reply=reply,
        episode_hint=EpisodeHint(
            should_create_episode=False,
            suggested_title=None,
            suggested_plan=None,
            priority="normal",
        ),
        raw_model_output={"fallback": True},
    )

def analyze_text_with_llm(req: AnalyzeRequest) -> AnalyzeResponse:
    """
    mode == chat ì¸ ê²½ìš° ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜.
    """
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="textê°€ ë¹„ì–´ ìˆì–´. ë¶„ì„í•  ë‚´ìš©ì´ ì—†ì–´.")

    # 1) LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ Gemini í˜¸ì¶œ (ì—ëŸ¬ ë‚˜ë©´ ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ìë™ ì „í™˜)
    if is_llm_available():
        try:
            combined_prompt = assemble_prompt_for_llm(req)
        except Exception:
            logger.exception("í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ ì¤‘ ì—ëŸ¬ ë°œìƒ - ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ì „í™˜")
            return rule_based_analyze(req)

        try:
            model = get_gemini_model()
            chat_session = model.start_chat()
            chat = chat_session.send_message(combined_prompt)

            ai_text = (chat.text or "").strip()
            if not ai_text:
                raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŒ")

            return AnalyzeResponse(
                ok=True,
                mode=req.mode,
                summary=(text[:80] + "...") if len(text) > 80 else text,
                intent="casual_chat",  # ë‚˜ì¤‘ì— intent ë¶„ì„ ë¶™ì´ë©´ ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨
                reply=ai_text,
                episode_hint=EpisodeHint(
                    should_create_episode=False,
                    suggested_title=None,
                    suggested_plan=None,
                    priority="normal",
                ),
                raw_model_output={
                    "combined_prompt": combined_prompt,
                },
            )

        except Exception:
            logger.exception("LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ - ê·œì¹™ ê¸°ë°˜ fallbackìœ¼ë¡œ ì „í™˜")
            return rule_based_analyze(req)

    # 2) LLM ì‚¬ìš© ë¶ˆê°€ ì‹œ, ê·œì¹™ ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©
    return rule_based_analyze(req)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”ë“œí¬ì¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.post("/director/analyze", response_model=AnalyzeResponse)
def analyze(req: AnalyzeRequest):
    """
    ê³µí†µ ë‡Œ ì—”ë“œí¬ì¸íŠ¸.
    ì§€ê¸ˆì€ mode == chat (í…ìŠ¤íŠ¸)ë§Œ ì§€ì›.
    """
    # vision ëª¨ë“œ ë¶„ê¸°
    if req.mode == "vision":
        return analyze_vision(req)

    # ê¸°ë³¸: í…ìŠ¤íŠ¸(chat) ë¶„ì„
    return analyze_text_with_llm(req)